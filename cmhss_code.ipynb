{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cmhss_code.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7c1sAM2MqAu"
      },
      "source": [
        "**References**\n",
        "\n",
        "For inspiration, we consulted the following website which discusses the use of Convolutional Neural Networks using Pytorch to analyse sentiment in Tweets: https://www.kaggle.com/youben/twitter-sentiment-analysis-using-cnn\n",
        "\n",
        "The Twitter Sentiment Corpus used was accessed here: http://thinknook.com/twitter-sentiment-analysis-training-corpus-dataset-2012-09-22/\n",
        "\n",
        "The following Python Notebook was used to help with CoreNLP setup: https://colab.research.google.com/github/stanfordnlp/stanza/blob/master/demo/Stanza_CoreNLP_Interface.ipynb\n",
        "\n",
        "CoreNLP can be found: https://stanfordnlp.github.io/CoreNLP/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUq9AT5tIyr2"
      },
      "source": [
        "# Mounting Google Drive\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nc_wuJrP-ua9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyDb_96HI8kG"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YK1Wyoui--WB"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "import datetime\n",
        "import copy\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "# ML Libraries\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Twitter\n",
        "import tweepy\n",
        "\n",
        "# Visualisation\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn import metrics\n",
        "\n",
        "# Global Parameters\n",
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download('punkt')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCquD5GSTWG7"
      },
      "source": [
        "# CoreNLP Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BaeCod6DVjHW"
      },
      "source": [
        "!wget https://nlp.stanford.edu/software/stanford-corenlp-full-2018-10-05.zip https://nlp.stanford.edu/software/stanford-english-corenlp-2018-10-05-models.jar"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FerlwH8kVvM1"
      },
      "source": [
        "# !unzip stanford-corenlp-full-2018-10-05.zip\n",
        "!mv stanford-english-corenlp-2018-10-05-models.jar /content/drive/MyDrive/Colab\\ Notebooks/cmhss/stanford-corenlp-full"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-BCO0O3Xhbj"
      },
      "source": [
        "# Install stanza; note that the prefix \"!\" is not needed if you are running in a terminal\n",
        "!pip install stanza\n",
        "\n",
        "# Import stanza\n",
        "import stanza"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJC-TfSufJ2I"
      },
      "source": [
        "# Downloading CoreNLP package with Stanza's installation command\n",
        "corenlp_dir = './corenlp'\n",
        "stanza.install_corenlp(dir=corenlp_dir)\n",
        "\n",
        "# Set the CORENLP_HOME environment variable to point to the installation location\n",
        "import os\n",
        "os.environ[\"CORENLP_HOME\"] = corenlp_dir"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5gtkNONfXXF"
      },
      "source": [
        "# Examine the CoreNLP installation folder to make sure the installation is successful\n",
        "!ls $CORENLP_HOME\n",
        " # Import client module\n",
        "from stanza.server import CoreNLPClient"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHzvgJIrhqI_"
      },
      "source": [
        "# Construct a CoreNLPClient with some basic annotators, a memory allocation of 4GB, and port number 9001\n",
        "client = CoreNLPClient(\n",
        "    annotators=['tokenize','sentiment'], \n",
        "    memory='4G', \n",
        "    endpoint='http://localhost:9002',\n",
        "    be_quiet=True)\n",
        "\n",
        "\n",
        "# Start the background server and wait for some time\n",
        "# Note that in practice this is totally optional, as by default the server will be started when the first annotation is performed\n",
        "client.start()\n",
        "import time; time.sleep(10)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2_3CBl5kXDO"
      },
      "source": [
        "text = \"I don't understand british weather. Make up your mind!\"\n",
        "result = client.annotate(text,\n",
        "                   properties={\n",
        "                       'annotators': 'sentiment',\n",
        "                       'outputFormat': 'json',\n",
        "                   })\n",
        "\n",
        "total_sentiment = 0\n",
        "for s in result[\"sentences\"]:\n",
        "    total_sentiment+= int(s[\"sentimentValue\"])\n",
        "    print(\"{}: '{}': {} (Sentiment Value) {} (Sentiment)\".format(\n",
        "        s[\"index\"],\n",
        "        \" \".join([t[\"word\"] for t in s[\"tokens\"]]),\n",
        "        s[\"sentimentValue\"], s[\"sentiment\"]))\n",
        "print(\"avg sentiment: \", round(total_sentiment/len(result[\"sentences\"])))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAApI9IfJjwB"
      },
      "source": [
        "# Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYHI4GJvJFaB"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxa1j6OZ_tPX"
      },
      "source": [
        "def load_dataset(cols):\n",
        "    dataset = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/cmhss/sentiment_dataset.csv', encoding='latin-1') # Enter your file location\n",
        "    dataset.columns = cols\n",
        "    return dataset\n",
        "\n",
        "def filter_columns(dataset, cols):\n",
        "  for col in cols:\n",
        "      del dataset[col]\n",
        "  return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yq4oQcl7JqM6"
      },
      "source": [
        "## Processing Text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trCpjH-P_4Gx"
      },
      "source": [
        "def remove_url_and_tags(tweet):\n",
        "    # URLs\n",
        "    tweet = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', tweet, flags=re.MULTILINE)\n",
        "    # Hashtags and user tags\n",
        "    tweet = re.sub(r'\\@\\w+|\\#','', tweet)\n",
        "    return tweet\n",
        "\n",
        "def preprocess_tweet_text(tweet):\n",
        "    tweet.lower()\n",
        "    # URLs\n",
        "    tweet = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', tweet, flags=re.MULTILINE)\n",
        "    # Hashtags and user tags\n",
        "    tweet = re.sub(r'\\@\\w+|\\#','', tweet)\n",
        "    # Punctuation\n",
        "    tweet = tweet.translate(str.maketrans('', '', string.punctuation))\n",
        "    # Stopwords\n",
        "    tweet_tokens = word_tokenize(tweet)\n",
        "    filtered_words = [w for w in tweet_tokens if not w in stop_words]    \n",
        "    return \" \".join(filtered_words)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joAPCYUUJPjv"
      },
      "source": [
        "## TfIdf vectorisation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZkC8JNgEsSU"
      },
      "source": [
        "def get_feature_vector(train_fit):\n",
        "    vector = TfidfVectorizer(sublinear_tf=True)\n",
        "    vector.fit(train_fit)\n",
        "    return vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BosOrIA3HiL2"
      },
      "source": [
        "## CoreNLP Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RAoOUffRHhpK",
        "outputId": "57d33af5-f706-4e89-92dd-97d9bfde3c6a"
      },
      "source": [
        "def corenlp_sentimentality(text):\n",
        "    result = client.annotate(text,\n",
        "                   properties={\n",
        "                       'annotators': 'sentiment, ner',\n",
        "                       'outputFormat': 'json',\n",
        "                   })\n",
        "\n",
        "    total_sentiment = 0\n",
        "    for s in result[\"sentences\"]:\n",
        "        total_sentiment+= int(s[\"sentimentValue\"])\n",
        "        # print(\"{}: '{}': {} (Sentiment Value) {} (Sentiment)\".format(\n",
        "        #     s[\"index\"],\n",
        "        #     \" \".join([t[\"word\"] for t in s[\"tokens\"]]),\n",
        "        #     s[\"sentimentValue\"], s[\"sentiment\"]))\n",
        "    # Return Average Sentiment \n",
        "    return round(total_sentiment/len(result[\"sentences\"]))\n",
        "corenlp_sentimentality(\"Good weather today! We should go to the park.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MxRgN78Jyxv"
      },
      "source": [
        "# Main body of code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTxDQtQmK36s"
      },
      "source": [
        "## Data processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKvKVoxlEv23"
      },
      "source": [
        "# Load dataset\n",
        "dataset = load_dataset(['ItemID', 'Sentiment', 'SentimentText'])\n",
        "# Remove unwanted columns from dataset\n",
        "n_dataset = filter_columns(dataset, ['ItemID'])\n",
        "#Preprocess data\n",
        "dataset.text = dataset['SentimentText'].apply(preprocess_tweet_text)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TOUXUogK8U9"
      },
      "source": [
        "## Machine Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHAlU6naAXoJ"
      },
      "source": [
        "# Tf-Idf vector\n",
        "tf_vector = get_feature_vector(np.array(dataset.iloc[:, 1]).ravel())\n",
        "tf_vector_test = copy.deepcopy(tf_vector)\n",
        "print(type(tf_vector))\n",
        "X = tf_vector.transform(np.array(dataset.iloc[:, 1]).ravel())\n",
        "y = np.array(dataset.iloc[:, 0]).ravel()\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=30)\n",
        "\n",
        "# Training Naive Bayes model\n",
        "NB_model = MultinomialNB()\n",
        "NB_model.fit(X_train, y_train)\n",
        "y_predict_nb = NB_model.predict(X_test)\n",
        "\n",
        "# Training Logistics Regression model\n",
        "LR_model = LogisticRegression(solver='lbfgs')\n",
        "LR_model.fit(X_train, y_train)\n",
        "y_predict_lr = LR_model.predict(X_test)\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUocvyBUL3Wb"
      },
      "source": [
        "## Twitter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzAFHoAKMrwl"
      },
      "source": [
        "### Authentication"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3CrYaxfM35-"
      },
      "source": [
        "consumerKey = \"rcSfK0JXDksoizx0WJsH56rPm\"\n",
        "consumerSecret = \"mY229tSJLfnk9AXPlq50KgEijMHVvEVUjuybFQVNuzhC5w7bx3\"\n",
        "accessToken = \"1394330176142598145-gpfb70iOBRXcE2DwiUxT3Pp7H04N2p\"\n",
        "accessTokenSecret = \"x5TSRfGal9QwVk9bfCJWb5wHoE2nT1h97nWwMEsgnJHnk\"\n",
        "auth = tweepy.OAuthHandler(consumerKey, consumerSecret)\n",
        "auth.set_access_token(accessToken, accessTokenSecret)\n",
        "api = tweepy.API(auth)\n",
        "# test authentication\n",
        "try:\n",
        "    api.verify_credentials()\n",
        "    print(\"Authentication OK\")\n",
        "except:\n",
        "    print(\"Error during authentication\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0xNktqpMxXE"
      },
      "source": [
        "### Collecting Tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-e3W5pcr-0Qx"
      },
      "source": [
        "Current_Date = datetime.datetime.today().strftime('%Y-%m-%d') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvSuXwO1_ZaM"
      },
      "source": [
        "print(datetime.date.today() - datetime.timedelta(days=0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1Lxu5wsNxWT"
      },
      "source": [
        "df_pos_neg_over_time = pd.DataFrame(columns=['date', 'sentiment', 'percentage'])\n",
        "df_pos_neg_over_time_corenlp = pd.DataFrame(columns=['date', 'sentiment', 'percentage'])\n",
        "df_pos_neg_over_time_corenlp_trunc = pd.DataFrame(columns=['date', 'sentiment', 'percentage'])\n",
        "for i in range(8):\n",
        "\n",
        "    days_before_today = i\n",
        "\n",
        "    # Number of Tweets\n",
        "    count = 10\n",
        "\n",
        "    # Date formatting\n",
        "    until_date_formatted = datetime.date.today() - datetime.timedelta(days=days_before_today)\n",
        "    day_before = until_date_formatted - datetime.timedelta(days=1)\n",
        "\n",
        "    # API request\n",
        "    tweets = tweepy.Cursor(api.search, q=\"london weather\", tweet_mode=\"extended\", until = until_date_formatted, lang = \"en\").items(15)\n",
        "\n",
        "    # Extracting desired information from each tweet\n",
        "    date_list = []\n",
        "    tweets_list = []\n",
        "\n",
        "    for tweet in tweets:\n",
        "        # Filtering by date\n",
        "        if  day_before == tweet.created_at.date(): # here we take only tweets from the same date as yesterday in format YYYY-MM-DD\n",
        "            # Accessing to full_text is different between RT and normal tweet\n",
        "            if 'retweeted_status' in tweet._json:  # if it is a RT take full_text\n",
        "                full_text = tweet._json['retweeted_status']['full_text']\n",
        "                text_date = str(tweet.created_at.date())\n",
        "            else:  # if it is a normal tweet take the full_text\n",
        "                full_text = tweet.full_text\n",
        "                text_date = str(tweet.created_at.date())\n",
        "            tweets_list.append(full_text)\n",
        "            date_list.append(text_date)\n",
        "        else:\n",
        "          print(\"not valid date: \", tweet.created_at.date() )\n",
        "          print(\"attributes are \", tweet.full_text)\n",
        "\n",
        "    df_tweets_corenlp = pd.DataFrame(list(zip(date_list, tweets_list)), columns =['date', 'text'])\n",
        "    df_tweets_preprocessed = pd.DataFrame(list(zip(date_list, tweets_list)), columns =['date', 'text'])\n",
        "\n",
        "    # Creating text feature\n",
        "    df_tweets_preprocessed.text = (df_tweets_preprocessed[\"text\"]).apply(preprocess_tweet_text)\n",
        "    test_feature = tf_vector_test.transform(np.array(df_tweets_preprocessed.iloc[:, 1]).ravel())\n",
        "\n",
        "    # Using Logistic Regression model for prediction\n",
        "    test_prediction_lr = LR_model.predict(test_feature)\n",
        "    total_tweets = len(test_prediction_lr)\n",
        "    total_positive_lr = (test_prediction_lr==1).sum()\n",
        "    total_negative_lr = (test_prediction_lr==0).sum()\n",
        "    positive_percentage_lr = (total_positive_lr / total_tweets) * 100\n",
        "    negative_percentage_lr = (total_negative_lr / total_tweets) * 100\n",
        "\n",
        "    df_pos_neg_over_time.loc[2*i] = [day_before, \"positive\", positive_percentage_lr]\n",
        "    df_pos_neg_over_time.loc[2*i+1] = [day_before, \"negative\", negative_percentage_lr]\n",
        "\n",
        "\n",
        "    # CoreNLP\n",
        "    df_tweets_corenlp.text = (df_tweets_corenlp[\"text\"]).apply(remove_url_and_tags)\n",
        "    tweets_for_corenlp = df_tweets_corenlp['text'].tolist()\n",
        "    total_score_1 = 0\n",
        "    total_score_2 = 0\n",
        "    total_score_3 = 0\n",
        "    total_score_4 = 0\n",
        "    for processed_tweet in tweets_for_corenlp:\n",
        "        sentimentality_score = corenlp_sentimentality(processed_tweet)\n",
        "        if 0 <= sentimentality_score < 1.5:\n",
        "          total_score_1 += 1\n",
        "        elif 1.5 <= sentimentality_score < 2.5:\n",
        "          total_score_2 += 1\n",
        "        elif 2.5 <=sentimentality_score < 3.5:\n",
        "          total_score_3 += 1\n",
        "        elif 3.5 <= sentimentality_score <= 4:\n",
        "          total_score_4 += 1 \n",
        "    score1_percentage_corenlp = (total_score_1 / total_tweets) * 100\n",
        "    score2_percentage_corenlp = (total_score_2 / total_tweets) * 100\n",
        "    score3_percentage_corenlp = (total_score_3 / total_tweets) * 100\n",
        "    score4_percentage_corenlp = (total_score_4 / total_tweets) * 100\n",
        "    positive_percentage_corenlp = ((total_score_3 + total_score_4) / total_tweets) * 100\n",
        "    negative_percentage_corenlp = ((total_score_1 + total_score_2) / total_tweets) * 100\n",
        "\n",
        "    df_pos_neg_over_time_corenlp.loc[4*i] = [day_before, \"1\", score1_percentage_corenlp]\n",
        "    df_pos_neg_over_time_corenlp.loc[4*i+1] = [day_before, \"2\", score2_percentage_corenlp]\n",
        "    df_pos_neg_over_time_corenlp.loc[4*i+2] = [day_before, \"3\", score3_percentage_corenlp]\n",
        "    df_pos_neg_over_time_corenlp.loc[4*i+3] = [day_before, \"4\", score4_percentage_corenlp]\n",
        "\n",
        "    df_pos_neg_over_time_corenlp_trunc.loc[2*i] = [day_before, \"positive\", positive_percentage_corenlp]\n",
        "    df_pos_neg_over_time_corenlp_trunc.loc[2*i+1] = [day_before, \"negative\", negative_percentage_corenlp]\n",
        "    \n",
        "print(df_pos_neg_over_time_corenlp.head)\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEc1nGiTMg7z"
      },
      "source": [
        "### Processing Tweets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eY5QbuLFMotq"
      },
      "source": [
        "## Tweet Statistics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibF2cBC_4TLy"
      },
      "source": [
        "# Accuracy of CoreNLP\n",
        "test_text = n_dataset[\"SentimentText\"].head(500000).tolist()\n",
        "test_labels = n_dataset[\"Sentiment\"].head(500000).tolist()\n",
        "predict_labels = list(map(corenlp_sentimentality, test_text))\n",
        "for i in range(len(predict_labels)):\n",
        "    if predict_labels[i] < 2.5:\n",
        "        predict_labels[i] = 0\n",
        "    elif predict_labels[i] >=2.5:\n",
        "        predict_labels[i] = 1\n",
        "predict_labels = np.array(predict_labels)\n",
        "test_labels = np.array(test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buu1o4DlTcdk"
      },
      "source": [
        "corenlp_score = accuracy_score(test_labels, predict_labels)\n",
        "print(\"Accuracy: %.3f\" % corenlp_score) #LR_model.score(X_test, y_test)\n",
        "corenlp_precision = precision_score(test_labels, predict_labels, average='binary')\n",
        "print('Precision: %.3f' % corenlp_precision) # Appropriate when minimizing false positives is the focus.\n",
        "corenlp_recall = recall_score(test_labels, predict_labels, average='binary')\n",
        "print('Recall: %.3f' % corenlp_recall) # Appropriate when minimizing false negatives is the focus.\n",
        "corenlp_f1 = f1_score(test_labels, predict_labels, average='binary')\n",
        "print('F-Measure: %.3f' % corenlp_f1)\n",
        "\n",
        "plt.figure(figsize=(9,9))\n",
        "cm = metrics.confusion_matrix(test_labels, predict_labels)\n",
        "sns.heatmap(cm, annot=True, fmt='.2', linewidths=.5, square = True, cmap = 'Blues_r');\n",
        "plt.ylabel('Actual label');\n",
        "plt.xlabel('Predicted label');\n",
        "all_sample_title = 'Stanford CoreNLP'\n",
        "plt.title(all_sample_title, size = 15);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOshzXjEDfs9"
      },
      "source": [
        "print(\"Naive Bayes accuracy: %.3f\" % accuracy_score(y_test, y_predict_nb))\n",
        "print(\"\\nLogistic Regression:\")\n",
        "lr_score = accuracy_score(y_test, y_predict_lr)\n",
        "print(\"Accuracy: %.3f\" % lr_score) #LR_model.score(X_test, y_test)\n",
        "lr_precision = precision_score(y_test, y_predict_lr, average='binary')\n",
        "print('Precision: %.3f' % lr_precision) # Appropriate when minimizing false positives is the focus.\n",
        "lr_recall = recall_score(y_test, y_predict_lr, average='binary')\n",
        "print('Recall: %.3f' % lr_recall) # Appropriate when minimizing false negatives is the focus.\n",
        "lr_f1 = f1_score(y_test, y_predict_lr, average='binary')\n",
        "print('F-Measure: %.3f' % lr_f1)\n",
        "\n",
        "plt.figure(figsize=(9,9))\n",
        "cm = metrics.confusion_matrix(y_test, y_predict_lr)\n",
        "sns.heatmap(cm, annot=True, fmt='.2%', linewidths=.5, square = True, cmap = 'Blues_r');\n",
        "plt.ylabel('Actual label');\n",
        "plt.xlabel('Predicted label');\n",
        "all_sample_title = 'Logistic Regression'\n",
        "plt.title(all_sample_title, size = 15);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fdw8aHjPUAOt"
      },
      "source": [
        "### Bar chart"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJgWYqFtT_yT"
      },
      "source": [
        "df_pos_neg_over_time = df_pos_neg_over_time.set_index('date')\n",
        "# df_pos_neg_over_time.index = pd.to_datetime(df_pos_neg_over_time.index)\n",
        "df_pos_neg_over_time.set_index('sentiment', append=True)['percentage'].unstack().plot.bar(stacked=True,figsize=(10,8), title=\"Overall Sentiment as Determined by a Linear Regression Model\")\n",
        "\n",
        "df_pos_neg_over_time_corenlp = df_pos_neg_over_time_corenlp.set_index('date')\n",
        "# df_pos_neg_over_time.index = pd.to_datetime(df_pos_neg_over_time.index)\n",
        "df_pos_neg_over_time_corenlp.set_index('sentiment', append=True)['percentage'].unstack().plot.bar(stacked=True,figsize=(10,8), title=\"Overall Sentiment as Determined by the CoreNLP Library\")\n",
        "\n",
        "df_pos_neg_over_time_corenlp_trunc = df_pos_neg_over_time_corenlp_trunc.set_index('date')\n",
        "# df_pos_neg_over_time.index = pd.to_datetime(df_pos_neg_over_time.index)\n",
        "df_pos_neg_over_time_corenlp_trunc.set_index('sentiment', append=True)['percentage'].unstack().plot.bar(stacked=True,figsize=(10,8), title=\"Overall Sentiment as Determined by the CoreNLP Library\")\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}